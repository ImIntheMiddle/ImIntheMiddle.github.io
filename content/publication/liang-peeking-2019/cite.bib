@misc{liang_peeking_2019,
 abstract = {Deciphering human behaviors to predict their future paths/trajectories and what they would do from videos is important in many applications. Motivated by this idea, this paper studies predicting a pedestrian's future path jointly with future activities. We propose an end-to-end, multi-task learning system utilizing rich visual features about human behavioral information and interaction with their surroundings. To facilitate the training, the network is learned with an auxiliary task of predicting future location in which the activity will happen. Experimental results demonstrate our state-of-the-art performance over two public benchmarks on future trajectory prediction. Moreover, our method is able to produce meaningful future activity prediction in addition to the path. The result provides the first empirical evidence that joint modeling of paths and activities benefits future path prediction.},
 annote = {Comment: In CVPR 2019. Code, models and more results are available at: https://next.cs.cmu.edu/},
 author = {Liang, Junwei and Jiang, Lu and Niebles, Juan Carlos and Hauptmann, Alexander and Fei-Fei, Li},
 doi = {10.48550/arXiv.1902.03748},
 file = {Liang et al_2019_Peeking into the Future.pdf:G\:\\マイドライブ\\Zoterosync\Łiang et al_2019_Peeking into the Future.pdf:application/pdf},
 month = {May},
 note = {arXiv:1902.03748 [cs]},
 publisher = {arXiv},
 shorttitle = {Peeking into the Future},
 title = {Peeking into the Future: Predicting Future Person Activities and Locations in Videos},
 url = {http://arxiv.org/abs/1902.03748},
 urldate = {2023-09-22},
 year = {2019}
}
