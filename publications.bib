
@misc{hadfield-menell_cooperative_2016,
	title = {Cooperative {Inverse} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1606.03137},
	doi = {10.48550/arXiv.1606.03137},
	abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
	month = nov,
	year = {2016},
	note = {arXiv:1606.03137 [cs]},
	keywords = {CIRL},
	file = {Hadfield-Menell et al_2016_Cooperative Inverse Reinforcement Learning.pdf:C\:\\Users\\kemut\\OneDrive\\ドキュメント\\研究\\zotero\\Hadfield-Menell et al_2016_Cooperative Inverse Reinforcement Learning.pdf:application/pdf},
}

@misc{mangalam_goals_2020,
	title = {From {Goals}, {Waypoints} \& {Paths} {To} {Long} {Term} {Human} {Trajectory} {Forecasting}},
	url = {http://arxiv.org/abs/2012.01526},
	doi = {10.48550/arXiv.2012.01526},
	abstract = {Human trajectory forecasting is an inherently multi-modal problem. Uncertainty in future trajectories stems from two sources: (a) sources that are known to the agent but unknown to the model, such as long term goals and (b)sources that are unknown to both the agent \& the model, such as intent of other agents \& irreducible randomness indecisions. We propose to factorize this uncertainty into its epistemic \& aleatoric sources. We model the epistemic un-certainty through multimodality in long term goals and the aleatoric uncertainty through multimodality in waypoints\& paths. To exemplify this dichotomy, we also propose a novel long term trajectory forecasting setting, with prediction horizons upto a minute, an order of magnitude longer than prior works. Finally, we presentY-net, a scene com-pliant trajectory forecasting network that exploits the pro-posed epistemic \& aleatoric structure for diverse trajectory predictions across long prediction horizons.Y-net significantly improves previous state-of-the-art performance on both (a) The well studied short prediction horizon settings on the Stanford Drone \& ETH/UCY datasets and (b) The proposed long prediction horizon setting on the re-purposed Stanford Drone \& Intersection Drone datasets.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Mangalam, Karttikeya and An, Yang and Girase, Harshayu and Malik, Jitendra},
	month = dec,
	year = {2020},
	note = {arXiv:2012.01526 [cs]},
	annote = {Comment: 14 pages, 7 figures (including 2 GIFs)},
	file = {Mangalam et al_2020_From Goals, Waypoints & Paths To Long Term Human Trajectory Forecasting.pdf:G\:\\マイドライブ\\Zoterosync\\Mangalam et al_2020_From Goals, Waypoints & Paths To Long Term Human Trajectory Forecasting.pdf:application/pdf},
}

@misc{nishimura_viewbirdiformer_2022,
	title = {{ViewBirdiformer}: {Learning} to recover ground-plane crowd trajectories and ego-motion from a single ego-centric view},
	shorttitle = {{ViewBirdiformer}},
	url = {http://arxiv.org/abs/2210.06332},
	doi = {10.48550/arXiv.2210.06332},
	abstract = {We introduce a novel learning-based method for view birdification, the task of recovering ground-plane trajectories of pedestrians of a crowd and their observer in the same crowd just from the observed ego-centric video. View birdification becomes essential for mobile robot navigation and localization in dense crowds where the static background is hard to see and reliably track. It is challenging mainly for two reasons; i) absolute trajectories of pedestrians are entangled with the movement of the observer which needs to be decoupled from their observed relative movements in the ego-centric video, and ii) a crowd motion model describing the pedestrian movement interactions is specific to the scene yet unknown a priori. For this, we introduce a Transformer-based network referred to as ViewBirdiformer which implicitly models the crowd motion through self-attention and decomposes relative 2D movement observations onto the ground-plane trajectories of the crowd and the camera through cross-attention between views. Most important, ViewBirdiformer achieves view birdification in a single forward pass which opens the door to accurate real-time, always-on situational awareness. Extensive experimental results demonstrate that ViewBirdiformer achieves accuracy similar to or better than state-of-the-art with three orders of magnitude reduction in execution time.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Nishimura, Mai and Nobuhara, Shohei and Nishino, Ko},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06332 [cs]},
	file = {Nishimura et al_2022_ViewBirdiformer.pdf:G\:\\マイドライブ\\Zoterosync\\Nishimura et al_2022_ViewBirdiformer.pdf:application/pdf},
}

@misc{mangalam_disentangling_2020,
	title = {Disentangling {Human} {Dynamics} for {Pedestrian} {Locomotion} {Forecasting} with {Noisy} {Supervision}},
	url = {http://arxiv.org/abs/1911.01138},
	doi = {10.48550/arXiv.1911.01138},
	abstract = {We tackle the problem of Human Locomotion Forecasting, a task for jointly predicting the spatial positions of several keypoints on the human body in the near future under an egocentric setting. In contrast to the previous work that aims to solve either the task of pose prediction or trajectory forecasting in isolation, we propose a framework to unify the two problems and address the practically useful task of pedestrian locomotion prediction in the wild. Among the major challenges in solving this task is the scarcity of annotated egocentric video datasets with dense annotations for pose, depth, or egomotion. To surmount this difficulty, we use state-of-the-art models to generate (noisy) annotations and propose robust forecasting models that can learn from this noisy supervision. We present a method to disentangle the overall pedestrian motion into easier to learn subparts by utilizing a pose completion and a decomposition module. The completion module fills in the missing key-point annotations and the decomposition module breaks the cleaned locomotion down to global (trajectory) and local (pose keypoint movements). Further, with Quasi RNN as our backbone, we propose a novel hierarchical trajectory forecasting network that utilizes low-level vision domain specific signals like egomotion and depth to predict the global trajectory. Our method leads to state-of-the-art results for the prediction of human locomotion in the egocentric view. Project pade: https://karttikeya.github.io/publication/plf/},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Mangalam, Karttikeya and Adeli, Ehsan and Lee, Kuan-Hui and Gaidon, Adrien and Niebles, Juan Carlos},
	month = apr,
	year = {2020},
	note = {arXiv:1911.01138 [cs]},
	annote = {Comment: Accepted to WACV 2020 (Oral)},
	file = {Mangalam et al_2020_Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy.pdf:G\:\\マイドライブ\\Zoterosync\\Mangalam et al_2020_Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy.pdf:application/pdf},
}

@inproceedings{bhattacharyya_long-term_2018,
	address = {Salt Lake City, UT},
	title = {Long-{Term} {On}-board {Prediction} of {People} in {Traffic} {Scenes} {Under} {Uncertainty}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578539/},
	doi = {10.1109/CVPR.2018.00441},
	abstract = {Progress towards advanced systems for assisted and autonomous driving is leveraging recent advances in recognition and segmentation methods. Yet, we are still facing challenges in bringing reliable driving to inner cities, as those are composed of highly dynamic scenes observed from a moving platform at considerable speeds. Anticipation becomes a key element in order to react timely and prevent accidents. In this paper we argue that it is necessary to predict at least 1 second and we thus propose a new model that jointly predicts ego motion and people trajectories over such large time horizons. We pay particular attention to modeling the uncertainty of our estimates arising from the non-deterministic nature of natural trafﬁc scenes. Our experimental results show that it is indeed possible to predict people trajectories at the desired time horizons and that our uncertainty estimates are informative of the prediction error. We also show that both sequence modeling of trajectories as well as our novel method of long term odometry prediction are essential for best performance.},
	language = {en},
	urldate = {2023-08-02},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Bhattacharyya, Apratim and Fritz, Mario and Schiele, Bernt},
	month = jun,
	year = {2018},
	pages = {4194--4202},
	file = {Bhattacharyya et al. - 2018 - Long-Term On-board Prediction of People in Traffic.pdf:C\:\\Users\\kemut\\Zotero\\storage\\S5VZ7HIY\\Bhattacharyya et al. - 2018 - Long-Term On-board Prediction of People in Traffic.pdf:application/pdf},
}

@inproceedings{chang_argoverse_2019,
	address = {Long Beach, CA, USA},
	title = {Argoverse: {3D} {Tracking} and {Forecasting} {With} {Rich} {Maps}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Argoverse},
	url = {https://ieeexplore.ieee.org/document/8953693/},
	doi = {10.1109/CVPR.2019.00895},
	language = {en},
	urldate = {2023-08-02},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chang, Ming-Fang and Ramanan, Deva and Hays, James and Lambert, John and Sangkloy, Patsorn and Singh, Jagjeet and Bak, Slawomir and Hartnett, Andrew and Wang, De and Carr, Peter and Lucey, Simon},
	month = jun,
	year = {2019},
	pages = {8740--8749},
	file = {Chang et al. - 2019 - Argoverse 3D Tracking and Forecasting With Rich M.pdf:C\:\\Users\\kemut\\Zotero\\storage\\EU7XPH8H\\Chang et al. - 2019 - Argoverse 3D Tracking and Forecasting With Rich M.pdf:application/pdf},
}

@misc{gu_densetnt_2021,
	title = {{DenseTNT}: {End}-to-end {Trajectory} {Prediction} from {Dense} {Goal} {Sets}},
	shorttitle = {{DenseTNT}},
	url = {http://arxiv.org/abs/2108.09640},
	doi = {10.48550/arXiv.2108.09640},
	abstract = {Due to the stochasticity of human behaviors, predicting the future trajectories of road agents is challenging for autonomous driving. Recently, goal-based multi-trajectory prediction methods are proved to be effective, where they first score over-sampled goal candidates and then select a final set from them. However, these methods usually involve goal predictions based on sparse pre-defined anchors and heuristic goal selection algorithms. In this work, we propose an anchor-free and end-to-end trajectory prediction model, named DenseTNT, that directly outputs a set of trajectories from dense goal candidates. In addition, we introduce an offline optimization-based technique to provide multi-future pseudo-labels for our final online model. Experiments show that DenseTNT achieves state-of-the-art performance, ranking 1st on the Argoverse motion forecasting benchmark and being the 1st place winner of the 2021 Waymo Open Dataset Motion Prediction Challenge.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Gu, Junru and Sun, Chen and Zhao, Hang},
	month = nov,
	year = {2021},
	note = {arXiv:2108.09640 [cs]},
	annote = {Comment: Accepted to ICCV 2021},
	file = {Gu et al_2021_DenseTNT.pdf:G\:\\マイドライブ\\Zoterosync\\Gu et al_2021_DenseTNT.pdf:application/pdf},
}

@article{rempe_trace_nodate,
	title = {Trace and {Pace}: {Controllable} {Pedestrian} {Animation} via {Guided} {Trajectory} {Diffusion}},
	abstract = {We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are available on the project page.},
	language = {en},
	author = {Rempe, Davis and Luo, Zhengyi and Peng, Xue Bin and Yuan, Ye and Kitani, Kris and Kreis, Karsten and Fidler, Sanja},
	keywords = {TP},
	file = {Rempe et al. - Trace and Pace Controllable Pedestrian Animation .pdf:C\:\\Users\\kemut\\Zotero\\storage\\TJIYKFTA\\Rempe et al. - Trace and Pace Controllable Pedestrian Animation .pdf:application/pdf},
}

@misc{chandra_robusttp_2019,
	title = {{RobustTP}: {End}-to-{End} {Trajectory} {Prediction} for {Heterogeneous} {Road}-{Agents} in {Dense} {Traffic} with {Noisy} {Sensor} {Inputs}},
	shorttitle = {{RobustTP}},
	url = {http://arxiv.org/abs/1907.08752},
	abstract = {We present RobustTP, an end-to-end algorithm for predicting future trajectories of road-agents in dense traffic with noisy sensor input trajectories obtained from RGB cameras (either static or moving) through a tracking algorithm. In this case, we consider noise as the deviation from the ground truth trajectory. The amount of noise depends on the accuracy of the tracking algorithm. Our approach is designed for dense heterogeneous traffic, where the road agents corresponding to a mixture of buses, cars, scooters, bicycles, or pedestrians. RobustTP is an approach that first computes trajectories using a combination of a non-linear motion model and a deep learning-based instance segmentation algorithm. Next, these noisy trajectories are trained using an LSTM-CNN neural network architecture that models the interactions between road-agents in dense and heterogeneous traffic. Our trajectory prediction algorithm outperforms state-of-the-art methods for end-to-end trajectory prediction using sensor inputs. We achieve an improvement of upto 18\% in average displacement error and an improvement ofup to 35.5\% in final displacement error at the end of the prediction window (5 seconds) over the next best method. All experiments were set up on an Nvidia TiTan Xp GPU. Additionally, we release a software framework, TrackNPred. The framework consists of implementations of state-of-the-art tracking and trajectory prediction methods and tools to benchmark and evaluate them on real-world dense traffic datasets.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Chandra, Rohan and Bhattacharya, Uttaran and Roncal, Christian and Bera, Aniket and Manocha, Dinesh},
	month = jul,
	year = {2019},
	note = {arXiv:1907.08752 [cs]},
	file = {Chandra et al_2019_RobustTP.pdf:G\:\\マイドライブ\\Zoterosync\\Chandra et al_2019_RobustTP.pdf:application/pdf},
}

@misc{saadatnejad_are_2022,
	title = {Are socially-aware trajectory prediction models really socially-aware?},
	url = {http://arxiv.org/abs/2108.10879},
	abstract = {Our field has recently witnessed an arms race of neural network-based trajectory predictors. While these predictors are at the core of many applications such as autonomous navigation or pedestrian flow simulations, their adversarial robustness has not been carefully studied. In this paper, we introduce a socially-attended attack to assess the social understanding of prediction models in terms of collision avoidance. An attack is a small yet carefully-crafted perturbations to fail predictors. Technically, we define collision as a failure mode of the output, and propose hard- and soft-attention mechanisms to guide our attack. Thanks to our attack, we shed light on the limitations of the current models in terms of their social understanding. We demonstrate the strengths of our method on the recent trajectory prediction models. Finally, we show that our attack can be employed to increase the social understanding of state-of-the-art models. The code is available online: https://s-attack.github.io/},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Saadatnejad, Saeed and Bahari, Mohammadhossein and Khorsandi, Pedram and Saneian, Mohammad and Moosavi-Dezfooli, Seyed-Mohsen and Alahi, Alexandre},
	month = feb,
	year = {2022},
	note = {arXiv:2108.10879 [cs]},
	keywords = {social},
	file = {Saadatnejad et al_2022_Are socially-aware trajectory prediction models really socially-aware.pdf:G\:\\マイドライブ\\Zoterosync\\Saadatnejad et al_2022_Are socially-aware trajectory prediction models really socially-aware.pdf:application/pdf},
}

@misc{shi_motion_2023,
	title = {Motion {Transformer} with {Global} {Intention} {Localization} and {Local} {Movement} {Refinement}},
	url = {http://arxiv.org/abs/2209.13508},
	abstract = {Predicting multimodal future behavior of traffic participants is essential for robotic vehicles to make safe decisions. Existing works explore to directly predict future trajectories based on latent features or utilize dense goal candidates to identify agent's destinations, where the former strategy converges slowly since all motion modes are derived from the same feature while the latter strategy has efficiency issue since its performance highly relies on the density of goal candidates. In this paper, we propose Motion TRansformer (MTR) framework that models motion prediction as the joint optimization of global intention localization and local movement refinement. Instead of using goal candidates, MTR incorporates spatial intention priors by adopting a small set of learnable motion query pairs. Each motion query pair takes charge of trajectory prediction and refinement for a specific motion mode, which stabilizes the training process and facilitates better multimodal predictions. Experiments show that MTR achieves state-of-the-art performance on both the marginal and joint motion prediction challenges, ranking 1st on the leaderboards of Waymo Open Motion Dataset. The source code is available at https://github.com/sshaoshuai/MTR.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Shi, Shaoshuai and Jiang, Li and Dai, Dengxin and Schiele, Bernt},
	month = mar,
	year = {2023},
	note = {arXiv:2209.13508 [cs]},
	annote = {Comment: Accepted by NeurIPS 2022 as Oral Presentation},
	file = {Shi et al_2023_Motion Transformer with Global Intention Localization and Local Movement.pdf:G\:\\マイドライブ\\Zoterosync\\Shi et al_2023_Motion Transformer with Global Intention Localization and Local Movement.pdf:application/pdf},
}

@inproceedings{guo_end--end_2022,
	address = {New Orleans, LA, USA},
	title = {End-to-{End} {Trajectory} {Distribution} {Prediction} {Based} on {Occupancy} {Grid} {Maps}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879534/},
	doi = {10.1109/CVPR52688.2022.00228},
	abstract = {In this paper, we aim to forecast a future trajectory distribution of a moving agent in the real world, given the social scene images and historical trajectories. Yet, it is a challenging task because the ground-truth distribution is unknown and unobservable, while only one of its samples can be applied for supervising model learning, which is prone to bias. Most recent works focus on predicting diverse trajectories in order to cover all modes of the real distribution, but they may despise the precision and thus give too much credit to unrealistic predictions. To address the issue, we learn the distribution with symmetric cross-entropy using occupancy grid maps as an explicit and scene-compliant approximation to the ground-truth distribution, which can effectively penalize unlikely predictions. In specific, we present an inverse reinforcement learning based multi-modal trajectory distribution forecasting framework that learns to plan by an approximate value iteration network in an end-to-end manner. Besides, based on the predicted distribution, we generate a small set of representative trajectories through a differentiable Transformer-based network, whose attention mechanism helps to model the relations of trajectories. In experiments, our method achieves state-of-the-art performance on the Stanford Drone Dataset and Intersection Drone Dataset.},
	language = {en},
	urldate = {2023-08-02},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Guo, Ke and Liu, Wenxi and Pan, Jia},
	month = jun,
	year = {2022},
	pages = {2232--2241},
	file = {Guo et al. - 2022 - End-to-End Trajectory Distribution Prediction Base.pdf:C\:\\Users\\kemut\\Zotero\\storage\\ZYJMYJRL\\Guo et al. - 2022 - End-to-End Trajectory Distribution Prediction Base.pdf:application/pdf},
}

@misc{liu_bevfusion_2022,
	title = {{BEVFusion}: {Multi}-{Task} {Multi}-{Sensor} {Fusion} with {Unified} {Bird}'s-{Eye} {View} {Representation}},
	shorttitle = {{BEVFusion}},
	url = {http://arxiv.org/abs/2205.13542},
	abstract = {Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3\% higher mAP and NDS on 3D object detection and 13.6\% higher mIoU on BEV map segmentation, with 1.9x lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Liu, Zhijian and Tang, Haotian and Amini, Alexander and Yang, Xinyu and Mao, Huizi and Rus, Daniela and Han, Song},
	month = jun,
	year = {2022},
	note = {arXiv:2205.13542 [cs]},
	annote = {Comment: The first two authors contributed equally to this work. Project page: https://bevfusion.mit.edu},
	file = {Liu et al_2022_BEVFusion.pdf:G\:\\マイドライブ\\Zoterosync\\Liu et al_2022_BEVFusion.pdf:application/pdf},
}

@misc{yin_center-based_2021,
	title = {Center-based {3D} {Object} {Detection} and {Tracking}},
	url = {http://arxiv.org/abs/2006.11275},
	abstract = {Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have difficulties enumerating all orientations or fitting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. Our framework, CenterPoint, first detects centers of objects using a keypoint detector and regresses to other attributes, including 3D size, 3D orientation, and velocity. In a second stage, it refines these estimates using additional point features on the object. In CenterPoint, 3D object tracking simplifies to greedy closest-point matching. The resulting detection and tracking algorithm is simple, efficient, and effective. CenterPoint achieved state-of-the-art performance on the nuScenes benchmark for both 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single model. On the Waymo Open Dataset, CenterPoint outperforms all previous single model method by a large margin and ranks first among all Lidar-only submissions. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Yin, Tianwei and Zhou, Xingyi and Krähenbühl, Philipp},
	month = jan,
	year = {2021},
	note = {arXiv:2006.11275 [cs]},
	annote = {Comment: update nuScenes and Waymo results},
	file = {Full Text PDF:C\:\\Users\\kemut\\Zotero\\storage\\KAZP34CT\\Yin et al. - 2021 - Center-based 3D Object Detection and Tracking.pdf:application/pdf;Yin et al_2021_Center-based 3D Object Detection and Tracking.pdf:G\:\\マイドライブ\\Zoterosync\\Yin et al_2021_Center-based 3D Object Detection and Tracking.pdf:application/pdf},
}

@misc{hu_planning-oriented_2023,
	title = {Planning-oriented {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2212.10156},
	doi = {10.48550/arXiv.2212.10156},
	abstract = {Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Hu, Yihan and Yang, Jiazhi and Chen, Li and Li, Keyu and Sima, Chonghao and Zhu, Xizhou and Chai, Siqi and Du, Senyao and Lin, Tianwei and Wang, Wenhai and Lu, Lewei and Jia, Xiaosong and Liu, Qiang and Dai, Jifeng and Qiao, Yu and Li, Hongyang},
	month = mar,
	year = {2023},
	note = {arXiv:2212.10156 [cs]},
	keywords = {BEV},
	annote = {Comment: CVPR 2023 award candidate. Project page: https://opendrivelab.github.io/UniAD/},
	file = {Hu et al_2023_Planning-oriented Autonomous Driving.pdf:G\:\\マイドライブ\\Zoterosync\\Hu et al_2023_Planning-oriented Autonomous Driving.pdf:application/pdf},
}

@misc{gu_vip3d_2023,
	title = {{ViP3D}: {End}-to-end {Visual} {Trajectory} {Prediction} via {3D} {Agent} {Queries}},
	shorttitle = {{ViP3D}},
	url = {http://arxiv.org/abs/2208.01582},
	doi = {10.48550/arXiv.2208.01582},
	abstract = {Perception and prediction are two separate modules in the existing autonomous driving systems. They interact with each other via hand-picked features such as agent bounding boxes and trajectories. Due to this separation, prediction, as a downstream module, only receives limited information from the perception module. To make matters worse, errors from the perception modules can propagate and accumulate, adversely affecting the prediction results. In this work, we propose ViP3D, a query-based visual trajectory prediction pipeline that exploits rich information from raw videos to directly predict future trajectories of agents in a scene. ViP3D employs sparse agent queries to detect, track, and predict throughout the pipeline, making it the first fully differentiable vision-based trajectory prediction approach. Instead of using historical feature maps and trajectories, useful information from previous timestamps is encoded in agent queries, which makes ViP3D a concise streaming prediction method. Furthermore, extensive experimental results on the nuScenes dataset show the strong vision-based prediction performance of ViP3D over traditional pipelines and previous end-to-end models.},
	urldate = {2023-08-05},
	publisher = {arXiv},
	author = {Gu, Junru and Hu, Chenxu and Zhang, Tianyuan and Chen, Xuanyao and Wang, Yilun and Wang, Yue and Zhao, Hang},
	month = jun,
	year = {2023},
	note = {arXiv:2208.01582 [cs]},
	annote = {Comment: CVPR 2023},
	file = {Gu et al_2023_ViP3D.pdf:G\:\\マイドライブ\\GoodNotes\\研究\\Gu et al_2023_ViP3D.pdf:application/pdf},
}

@misc{wang_navistar_2023,
	title = {{NaviSTAR}: {Socially} {Aware} {Robot} {Navigation} with {Hybrid} {Spatio}-{Temporal} {Graph} {Transformer} and {Preference} {Learning}},
	shorttitle = {{NaviSTAR}},
	url = {http://arxiv.org/abs/2304.05979},
	doi = {10.48550/arXiv.2304.05979},
	abstract = {Developing robotic technologies for use in human society requires ensuring the safety of robots' navigation behaviors while adhering to pedestrians' expectations and social norms. However, maintaining real-time communication between robots and pedestrians to avoid collisions can be challenging. To address these challenges, we propose a novel socially-aware navigation benchmark called NaviSTAR, which utilizes a hybrid Spatio-Temporal grAph tRansformer (STAR) to understand interactions in human-rich environments fusing potential crowd multi-modal information. We leverage off-policy reinforcement learning algorithm with preference learning to train a policy and a reward function network with supervisor guidance. Additionally, we design a social score function to evaluate the overall performance of social navigation. To compare, we train and test our algorithm and other state-of-the-art methods in both simulator and real-world scenarios independently. Our results show that NaviSTAR outperforms previous methods with outstanding performance{\textbackslash}footnote\{\vphantom{\}}The source code and experiment videos of this work are available at: https://sites.google.com/view/san-navistar},
	urldate = {2023-08-05},
	publisher = {arXiv},
	author = {Wang, Weizheng and Wang, Ruiqi and Mao, Le and Min, Byung-Cheol},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05979 [cs]},
	file = {Wang et al_2023_NaviSTAR.pdf:G\:\\マイドライブ\\GoodNotes\\研究\\Wang et al_2023_NaviSTAR.pdf:application/pdf},
}

@article{guillen-ruiz_evolution_2023,
	title = {Evolution of {Socially}-{Aware} {Robot} {Navigation}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/7/1570},
	doi = {10.3390/electronics12071570},
	abstract = {In recent years, commercial and research interest in service robots working in everyday environments has grown. These devices are expected to move autonomously in crowded environments, maximizing not only movement efficiency and safety parameters, but also social acceptability. Extending traditional path planning modules with socially aware criteria, while maintaining fast algorithms capable of reacting to human behavior without causing discomfort, can be a complex challenge. Solving this challenge has involved the development of proactive systems that take into account cooperation (and not only interaction) with the people around them, the determined incorporation of approaches based on Deep Learning, or the recent fusion with skills coming from the field of human–robot interaction (speech, touch). This review analyzes approaches to socially aware navigation and classifies them according to the strategies followed by the robot to manage interaction (or cooperation) with humans.},
	language = {en},
	number = {7},
	urldate = {2023-08-05},
	journal = {Electronics},
	author = {Guillén-Ruiz, Silvia and Bandera, Juan Pedro and Hidalgo-Paniagua, Alejandro and Bandera, Antonio},
	month = jan,
	year = {2023},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {1570},
	file = {Guillén-Ruiz et al_2023_Evolution of Socially-Aware Robot Navigation.pdf:G\:\\マイドライブ\\GoodNotes\\研究\\Guillén-Ruiz et al_2023_Evolution of Socially-Aware Robot Navigation.pdf:application/pdf},
}

@misc{wang_feedback-efficient_2022,
	title = {Feedback-efficient {Active} {Preference} {Learning} for {Socially} {Aware} {Robot} {Navigation}},
	url = {http://arxiv.org/abs/2201.00469},
	abstract = {Socially aware robot navigation, where a robot is required to optimize its trajectory to maintain comfortable and compliant spatial interactions with humans in addition to reaching its goal without collisions, is a fundamental yet challenging task in the context of human-robot interaction. While existing learning-based methods have achieved better performance than the preceding model-based ones, they still have drawbacks: reinforcement learning depends on the handcrafted reward that is unlikely to effectively quantify broad social compliance, and can lead to reward exploitation problems; meanwhile, inverse reinforcement learning suffers from the need for expensive human demonstrations. In this paper, we propose a feedback-efficient active preference learning approach, FAPL, that distills human comfort and expectation into a reward model to guide the robot agent to explore latent aspects of social compliance. We further introduce hybrid experience learning to improve the efficiency of human feedback and samples, and evaluate benefits of robot behaviors learned from FAPL through extensive simulation experiments and a user study (N=10) employing a physical robot to navigate with human subjects in real-world scenarios. Source code and experiment videos for this work are available at:https://sites.google.com/view/san-fapl.},
	urldate = {2023-08-05},
	publisher = {arXiv},
	author = {Wang, Ruiqi and Wang, Weizheng and Min, Byung-Cheol},
	month = jul,
	year = {2022},
	note = {arXiv:2201.00469 [cs]
version: 2},
	annote = {Comment: To appear in IROS 2022},
	file = {Wang et al_2022_Feedback-efficient Active Preference Learning for Socially Aware Robot.pdf:G\:\\マイドライブ\\GoodNotes\\研究\\Wang et al_2022_Feedback-efficient Active Preference Learning for Socially Aware Robot.pdf:application/pdf},
}

@misc{moller_survey_2021,
	title = {A {Survey} on {Human}-aware {Robot} {Navigation}},
	url = {http://arxiv.org/abs/2106.11650},
	abstract = {Intelligent systems are increasingly part of our everyday lives and have been integrated seamlessly to the point where it is difficult to imagine a world without them. Physical manifestations of those systems on the other hand, in the form of embodied agents or robots, have so far been used only for specific applications and are often limited to functional roles (e.g. in the industry, entertainment and military fields). Given the current growth and innovation in the research communities concerned with the topics of robot navigation, human-robot-interaction and human activity recognition, it seems like this might soon change. Robots are increasingly easy to obtain and use and the acceptance of them in general is growing. However, the design of a socially compliant robot that can function as a companion needs to take various areas of research into account. This paper is concerned with the navigation aspect of a socially-compliant robot and provides a survey of existing solutions for the relevant areas of research as well as an outlook on possible future directions.},
	urldate = {2023-08-05},
	publisher = {arXiv},
	author = {Möller, Ronja and Furnari, Antonino and Battiato, Sebastiano and Härmä, Aki and Farinella, Giovanni Maria},
	month = jun,
	year = {2021},
	note = {arXiv:2106.11650 [cs]},
	annote = {Comment: Robotics and Autonomous Systems, 2021},
	file = {Möller et al_2021_A Survey on Human-aware Robot Navigation.pdf:G\:\\マイドライブ\\GoodNotes\\研究\\Möller et al_2021_A Survey on Human-aware Robot Navigation.pdf:application/pdf},
}

@misc{nishimura_view_2022,
	title = {View {Birdification} in the {Crowd}: {Ground}-{Plane} {Localization} from {Perceived} {Movements}},
	shorttitle = {View {Birdification} in the {Crowd}},
	url = {http://arxiv.org/abs/2111.05060},
	doi = {10.48550/arXiv.2111.05060},
	abstract = {We introduce view birdification, the problem of recovering ground-plane movements of people in a crowd from an ego-centric video captured from an observer (e.g., a person or a vehicle) also moving in the crowd. Recovered ground-plane movements would provide a sound basis for situational understanding and benefit downstream applications in computer vision and robotics. In this paper, we formulate view birdification as a geometric trajectory reconstruction problem and derive a cascaded optimization method from a Bayesian perspective. The method first estimates the observer's movement and then localizes surrounding pedestrians for each frame while taking into account the local interactions between them. We introduce three datasets by leveraging synthetic and real trajectories of people in crowds and evaluate the effectiveness of our method. The results demonstrate the accuracy of our method and set the ground for further studies of view birdification as an important but challenging visual understanding problem.},
	urldate = {2023-08-05},
	publisher = {arXiv},
	author = {Nishimura, Mai and Nobuhara, Shohei and Nishino, Ko},
	month = oct,
	year = {2022},
	note = {arXiv:2111.05060 [cs]
version: 2},
	annote = {Comment: Extended journal version of the original paper at BMVC 2021},
	file = {Nishimura et al_2022_View Birdification in the Crowd.pdf:G\:\\マイドライブ\\GoodNotes\\研究\\Nishimura et al_2022_View Birdification in the Crowd.pdf:application/pdf},
}

@article{__2021,
	title = {胸部方向を考慮した車載映像における歩行者の経路予測},
	volume = {2021-CVIM-226},
	issn = {2188-8701},
	url = {https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=211179&item_no=1},
	abstract = {情報学広場 情報処理学会電子図書館},
	language = {ja},
	number = {31},
	urldate = {2023-08-06},
	journal = {研究報告コンピュータビジョンとイメージメディア（CVIM）},
	author = {太一, 福田 and 章平, 延原 and 恒, 西野},
	month = may,
	year = {2021},
	pages = {1--8},
	file = {太一 et al_2021_胸部方向を考慮した車載映像における歩行者の経路予測.pdf:G\:\\マイドライブ\\GoodNotes\\研究\\太一 et al_2021_胸部方向を考慮した車載映像における歩行者の経路予測.pdf:application/pdf},
}

@article{rempe_trace_nodate-1,
	title = {Trace and {Pace}: {Controllable} {Pedestrian} {Animation} via {Guided} {Trajectory} {Diffusion}},
	abstract = {We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are available on the project page.},
	language = {en},
	author = {Rempe, Davis and Luo, Zhengyi and Peng, Xue Bin and Yuan, Ye and Kitani, Kris and Kreis, Karsten and Fidler, Sanja},
	file = {Rempe et al. - Trace and Pace Controllable Pedestrian Animation .pdf:C\:\\Users\\kemut\\Zotero\\storage\\KHUV8MUH\\Rempe et al. - Trace and Pace Controllable Pedestrian Animation .pdf:application/pdf},
}

@misc{noauthor__nodate,
	title = {受信トレイ (276) - sd23426@tti-j.net - 豊田工業大学 メール},
	url = {https://mail.google.com/mail/u/3/#inbox?projector=1},
	urldate = {2023-08-10},
	file = {受信トレイ (276) - sd23426@tti-j.net - 豊田工業大学 メール:C\:\\Users\\kemut\\Zotero\\storage\\ZBA7TN59\\3.html:text/html},
}

@article{rempe_trace_nodate-2,
	title = {Trace and {Pace}: {Controllable} {Pedestrian} {Animation} via {Guided} {Trajectory} {Diffusion}},
	abstract = {We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain. Video results are available on the project page.},
	language = {en},
	author = {Rempe, Davis and Luo, Zhengyi and Peng, Xue Bin and Yuan, Ye and Kitani, Kris and Kreis, Karsten and Fidler, Sanja},
	file = {Rempe et al. - Trace and Pace Controllable Pedestrian Animation .pdf:C\:\\Users\\kemut\\Zotero\\storage\\W4GZPHIQ\\Rempe et al. - Trace and Pace Controllable Pedestrian Animation .pdf:application/pdf},
}

@misc{wu_multiagent_2023,
	title = {Multiagent {Inverse} {Reinforcement} {Learning} via {Theory} of {Mind} {Reasoning}},
	url = {http://arxiv.org/abs/2302.10238},
	doi = {10.5555/3545946.3598703},
	abstract = {We approach the problem of understanding how people interact with each other in collaborative settings, especially when individuals know little about their teammates, via Multiagent Inverse Reinforcement Learning (MIRL), where the goal is to infer the reward functions guiding the behavior of each individual given trajectories of a team's behavior during some task. Unlike current MIRL approaches, we do not assume that team members know each other's goals a priori; rather, that they collaborate by adapting to the goals of others perceived by observing their behavior, all while jointly performing a task. To address this problem, we propose a novel approach to MIRL via Theory of Mind (MIRL-ToM). For each agent, we first use ToM reasoning to estimate a posterior distribution over baseline reward profiles given their demonstrated behavior. We then perform MIRL via decentralized equilibrium by employing single-agent Maximum Entropy IRL to infer a reward function for each agent, where we simulate the behavior of other teammates according to the time-varying distribution over profiles. We evaluate our approach in a simulated 2-player search-and-rescue operation where the goal of the agents, playing different roles, is to search for and evacuate victims in the environment. Our results show that the choice of baseline profiles is paramount to the recovery of the ground-truth rewards, and that MIRL-ToM is able to recover the rewards used by agents interacting both with known and unknown teammates.},
	urldate = {2023-08-16},
	author = {Wu, Haochen and Sequeira, Pedro and Pynadath, David V.},
	month = mar,
	year = {2023},
	note = {arXiv:2302.10238 [cs]},
	annote = {Comment: Accepted as a full paper at AAMAS2023},
	file = {Wu et al_2023_Multiagent Inverse Reinforcement Learning via Theory of Mind Reasoning.pdf:G\:\\マイドライブ\\Zoterosync\\Wu et al_2023_Multiagent Inverse Reinforcement Learning via Theory of Mind Reasoning.pdf:application/pdf},
}

@misc{liu_group_2023,
	title = {Group {Pose}: {A} {Simple} {Baseline} for {End}-to-{End} {Multi}-person {Pose} {Estimation}},
	shorttitle = {Group {Pose}},
	url = {http://arxiv.org/abs/2308.07313},
	doi = {10.48550/arXiv.2308.07313},
	abstract = {In this paper, we study the problem of end-to-end multi-person pose estimation. State-of-the-art solutions adopt the DETR-like framework, and mainly develop the complex decoder, e.g., regarding pose estimation as keypoint box detection and combining with human detection in ED-Pose, hierarchically predicting with pose decoder and joint (keypoint) decoder in PETR. We present a simple yet effective transformer approach, named Group Pose. We simply regard \$K\$-keypoint pose estimation as predicting a set of \$N{\textbackslash}times K\$ keypoint positions, each from a keypoint query, as well as representing each pose with an instance query for scoring \$N\$ pose predictions. Motivated by the intuition that the interaction, among across-instance queries of different types, is not directly helpful, we make a simple modification to decoder self-attention. We replace single self-attention over all the \$N{\textbackslash}times(K+1)\$ queries with two subsequent group self-attentions: (i) \$N\$ within-instance self-attention, with each over \$K\$ keypoint queries and one instance query, and (ii) \$(K+1)\$ same-type across-instance self-attention, each over \$N\$ queries of the same type. The resulting decoder removes the interaction among across-instance type-different queries, easing the optimization and thus improving the performance. Experimental results on MS COCO and CrowdPose show that our approach without human box supervision is superior to previous methods with complex decoders, and even is slightly better than ED-Pose that uses human box supervision. \${\textbackslash}href\{https://github.com/Michel-liu/GroupPose-Paddle\}\{{\textbackslash}rm Paddle\}\$ and \${\textbackslash}href\{https://github.com/Michel-liu/GroupPose\}\{{\textbackslash}rm PyTorch\}\$ code are available.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Liu, Huan and Chen, Qiang and Tan, Zichang and Liu, Jiang-Jiang and Wang, Jian and Su, Xiangbo and Li, Xiaolong and Yao, Kun and Han, Junyu and Ding, Errui and Zhao, Yao and Wang, Jingdong},
	month = aug,
	year = {2023},
	note = {arXiv:2308.07313 [cs]
version: 1},
	annote = {Comment: Accepted by ICCV 2023},
	file = {Liu et al_2023_Group Pose.pdf:G\:\\マイドライブ\\Zoterosync\\Liu et al_2023_Group Pose.pdf:application/pdf},
}

@misc{xu_eqmotion_2023,
	title = {{EqMotion}: {Equivariant} {Multi}-agent {Motion} {Prediction} with {Invariant} {Interaction} {Reasoning}},
	shorttitle = {{EqMotion}},
	url = {http://arxiv.org/abs/2303.10876},
	doi = {10.48550/arXiv.2303.10876},
	abstract = {Learning to predict agent motions with relationship reasoning is important for many applications. In motion prediction tasks, maintaining motion equivariance under Euclidean geometric transformations and invariance of agent interaction is a critical and fundamental principle. However, such equivariance and invariance properties are overlooked by most existing methods. To fill this gap, we propose EqMotion, an efficient equivariant motion prediction model with invariant interaction reasoning. To achieve motion equivariance, we propose an equivariant geometric feature learning module to learn a Euclidean transformable feature through dedicated designs of equivariant operations. To reason agent's interactions, we propose an invariant interaction reasoning module to achieve a more stable interaction modeling. To further promote more comprehensive motion features, we propose an invariant pattern feature learning module to learn an invariant pattern feature, which cooperates with the equivariant geometric feature to enhance network expressiveness. We conduct experiments for the proposed model on four distinct scenarios: particle dynamics, molecule dynamics, human skeleton motion prediction and pedestrian trajectory prediction. Experimental results show that our method is not only generally applicable, but also achieves state-of-the-art prediction performances on all the four tasks, improving by 24.0/30.1/8.6/9.2\%. Code is available at https://github.com/MediaBrain-SJTU/EqMotion.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Xu, Chenxin and Tan, Robby T. and Tan, Yuhong and Chen, Siheng and Wang, Yu Guang and Wang, Xinchao and Wang, Yanfeng},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10876 [cs]},
	annote = {Comment: Accepted to CVPR 2023},
	file = {Xu et al_2023_EqMotion.pdf:G\:\\マイドライブ\\Zoterosync\\Xu et al_2023_EqMotion.pdf:application/pdf},
}

@misc{liang_peeking_2019,
	title = {Peeking into the {Future}: {Predicting} {Future} {Person} {Activities} and {Locations} in {Videos}},
	shorttitle = {Peeking into the {Future}},
	url = {http://arxiv.org/abs/1902.03748},
	doi = {10.48550/arXiv.1902.03748},
	abstract = {Deciphering human behaviors to predict their future paths/trajectories and what they would do from videos is important in many applications. Motivated by this idea, this paper studies predicting a pedestrian's future path jointly with future activities. We propose an end-to-end, multi-task learning system utilizing rich visual features about human behavioral information and interaction with their surroundings. To facilitate the training, the network is learned with an auxiliary task of predicting future location in which the activity will happen. Experimental results demonstrate our state-of-the-art performance over two public benchmarks on future trajectory prediction. Moreover, our method is able to produce meaningful future activity prediction in addition to the path. The result provides the first empirical evidence that joint modeling of paths and activities benefits future path prediction.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Liang, Junwei and Jiang, Lu and Niebles, Juan Carlos and Hauptmann, Alexander and Fei-Fei, Li},
	month = may,
	year = {2019},
	note = {arXiv:1902.03748 [cs]},
	annote = {Comment: In CVPR 2019. Code, models and more results are available at: https://next.cs.cmu.edu/},
	file = {Liang et al_2019_Peeking into the Future.pdf:G\:\\マイドライブ\\Zoterosync\\Liang et al_2019_Peeking into the Future.pdf:application/pdf},
}

@misc{zhang_body_2023,
	title = {Body {Knowledge} and {Uncertainty} {Modeling} for {Monocular} {3D} {Human} {Body} {Reconstruction}},
	url = {http://arxiv.org/abs/2308.00799},
	doi = {10.48550/arXiv.2308.00799},
	abstract = {While 3D body reconstruction methods have made remarkable progress recently, it remains difficult to acquire the sufficiently accurate and numerous 3D supervisions required for training. In this paper, we propose {\textbackslash}textbf\{KNOWN\}, a framework that effectively utilizes body {\textbackslash}textbf\{KNOW\}ledge and u{\textbackslash}textbf\{N\}certainty modeling to compensate for insufficient 3D supervisions. KNOWN exploits a comprehensive set of generic body constraints derived from well-established body knowledge. These generic constraints precisely and explicitly characterize the reconstruction plausibility and enable 3D reconstruction models to be trained without any 3D data. Moreover, existing methods typically use images from multiple datasets during training, which can result in data noise ({\textbackslash}textit\{e.g.\}, inconsistent joint annotation) and data imbalance ({\textbackslash}textit\{e.g.\}, minority images representing unusual poses or captured from challenging camera views). KNOWN solves these problems through a novel probabilistic framework that models both aleatoric and epistemic uncertainty. Aleatoric uncertainty is encoded in a robust Negative Log-Likelihood (NLL) training loss, while epistemic uncertainty is used to guide model refinement. Experiments demonstrate that KNOWN's body reconstruction outperforms prior weakly-supervised approaches, particularly on the challenging minority images.},
	urldate = {2023-10-09},
	publisher = {arXiv},
	author = {Zhang, Yufei and Wang, Hanjing and Kephart, Jeffrey O. and Ji, Qiang},
	month = aug,
	year = {2023},
	note = {arXiv:2308.00799 [cs]},
	annote = {Comment: ICCV 2023},
	file = {Zhang et al_2023_Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body.pdf:G\:\\マイドライブ\\Zoterosync\\Zhang et al_2023_Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body.pdf:application/pdf},
}

@misc{taketsugu_active_2023,
	title = {Active {Transfer} {Learning} for {Efficient} {Video}-{Specific} {Human} {Pose} {Estimation}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2311.05041},
	doi = {10.48550/arXiv.2311.05041},
	abstract = {Human Pose (HP) estimation is actively researched because of its wide range of applications. However, even estimators pre-trained on large datasets may not perform satisfactorily due to a domain gap between the training and test data. To address this issue, we present our approach combining Active Learning (AL) and Transfer Learning (TL) to adapt HP estimators to individual video domains efficiently. For efficient learning, our approach quantifies (i) the estimation uncertainty based on the temporal changes in the estimated heatmaps and (ii) the unnaturalness in the estimated full-body HPs. These quantified criteria are then effectively combined with the state-of-the-art representativeness criterion to select uncertain and diverse samples for efficient HP estimator learning. Furthermore, we reconsider the existing Active Transfer Learning (ATL) method to introduce novel ideas related to the retraining methods and Stopping Criteria (SC). Experimental results demonstrate that our method enhances learning efficiency and outperforms comparative methods. Our code is publicly available at: https://github.com/ImIntheMiddle/VATL4Pose-WACV2024},
	urldate = {2023-11-24},
	publisher = {arXiv},
	author = {Taketsugu, Hiromu and Ukita, Norimichi},
	month = nov,
	year = {2023},
	note = {arXiv:2311.05041 [cs]},
	annote = {Comment: 17 pages, 12 figures, Accepted by WACV 2024},
	file = {Taketsugu_Ukita_2023_Active Transfer Learning for Efficient Video-Specific Human Pose Estimation.pdf:G\:\\マイドライブ\\Zoterosync\\Taketsugu_Ukita_2023_Active Transfer Learning for Efficient Video-Specific Human Pose Estimation.pdf:application/pdf},
}

@inproceedings{taketsugu_uncertainty_2023,
	title = {Uncertainty {Criteria} in {Active} {Transfer} {Learning} for {Efficient} {Video}-{Specific} {Human} {Pose} {Estimation}},
	copyright = {All rights reserved},
	url = {https://ieeexplore.ieee.org/abstract/document/10215565},
	doi = {10.23919/MVA57639.2023.10215565},
	abstract = {This paper presents a combination of Active Learning (AL) and Transfer Learning (TL) for efficiently adapting Human Pose (HP) estimators to individual videos. The proposed approach quantifies estimation uncertainty through the temporal changes and unnaturalness of estimated HPs. These uncertainty criteria are combined with clustering-based representativeness criterion to avoid the useless selection of similar samples. Experiments demonstrated that the proposed method achieves high learning efficiency and outperforms comparative methods.},
	urldate = {2023-11-24},
	booktitle = {2023 18th {International} {Conference} on {Machine} {Vision} and {Applications} ({MVA})},
	author = {Taketsugu, Hiromu and Ukita, Norimichi},
	month = jul,
	year = {2023},
	pages = {1--5},
	file = {Taketsugu_Ukita_2023_Uncertainty Criteria in Active Transfer Learning for Efficient Video-Specific.pdf:G\:\\マイドライブ\\Zoterosync\\Taketsugu_Ukita_2023_Uncertainty Criteria in Active Transfer Learning for Efficient Video-Specific.pdf:application/pdf},
}
